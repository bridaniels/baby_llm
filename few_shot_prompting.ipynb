{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Few-Shot Prompting \n",
    "--- \n",
    "### *Prompt Engineering*: process of crafting prompt text for a given model/parameters to enable the model to append to your prompt or continue your prompt. \n",
    "- Includes instructions, context, examples, and cue. <sup>[1](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=models-prompt-tips)</sup>\n",
    "    - *instruction*: imperative statement\n",
    "    - *context*: include information to guide model towards desired output \n",
    "    - *examples*: indicate desired format/shape of output via examples \n",
    "    - *que*: text at the end of the prompt likely to start the generated output on a desired path \n",
    "- technique to enable in-context learning where we provide demonstrations in the prompt to steer the model to better performance <sup>[2](https://www.promptingguide.ai/techniques/fewshot)</sup>\n",
    "\n",
    "### *Few-Shot Prompting*: provide examples when you are unable to properly articulate input for desired output but still want results\n",
    "- no instructions necessary, the examples show the model how you expect it to respond\n",
    "- random nature of the model = potentially different output each time <sup>[3](https://machinelearningmastery.com/what-are-zero-shot-prompting-and-few-shot-prompting/)</sup>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *zero-shot learning*: \n",
    "---\n",
    "### model learns how to classify classes it has not seen before <sup>[4](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab)</sup>\n",
    "- give model a prompt that is not part of the training data, and the model can create desired output \n",
    "    - *CAPTCHA image identification*\n",
    "- machine learning models would have to be fundamentally adapted before adding new classes/changing the output criteria, while large language models do not have to be retrained <sup>[3](https://machinelearningmastery.com/what-are-zero-shot-prompting-and-few-shot-prompting/)</sup>\n",
    "\n",
    "> ### Contrastive Language-Image Pretraining (CLIP)\n",
    "> - Goal: *Classify Images Without Any Explicit Labels*\n",
    "> - Two Stages: training stage and inference stage (same as supervised models)\n",
    ">   - *training stage*:  CLIP learns about images via corresponding auxiliary text \n",
    ">       - auxillary text is a form of supervision via data attributes, not labels (computationally expensive)<sup>[5](https://arxiv.org/pdf/2106.02869.pdf)</sup>\n",
    ">       - over time the model learns to extract more important information resulting in better output \n",
    ">   - *inference stage*: minimize difference between image encoding and corresponding text \n",
    ">       - encodings: lower-dimension representations of data, aka the most important/distinguishable information \n",
    ">       - model output = encodings of trained images   \n",
    ">       - expected output = text encoding of corresponding captions \n",
    ">\n",
    "> - *Contrastive Learning*: ML technique used to learn dataset features without labels <sup>[6](https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607)</sup>\n",
    ">   - model learns correlation of various data points \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References: \n",
    "1.  [IBM Watson Docs](https://www.ibm.com/docs/en/watsonx-as-a-service?topic=models-prompt-tips)\n",
    "2.  [Prompt Guide AI](https://www.promptingguide.ai/techniques/fewshot)\n",
    "3.  [MLM: What Are Zero-Shot Prompting and Few-Shot Prompting](https://machinelearningmastery.com/what-are-zero-shot-prompting-and-few-shot-prompting/)\n",
    "4.  [TDS: Understanding Zero-Shot Learning Making ML More Human](https://towardsdatascience.com/understanding-zero-shot-learning-making-ml-more-human-4653ac35ccab)\n",
    "5. [Integrading Auxiliary Information in Self-Supervised Learning](https://arxiv.org/pdf/2106.02869.pdf)\n",
    "6. [TDS: Understanding Contrastive Learning](https://towardsdatascience.com/understanding-contrastive-learning-d5b19fd96607)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
