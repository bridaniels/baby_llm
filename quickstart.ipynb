{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Large Language Models (LLMS)**\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Import Libraries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup OpenAI API Key \n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LLM: takes string as input and returns and string \n",
    "#ChatModels: takes a list of messages as input and returns a message \n",
    "\n",
    "llm = OpenAI(openai_api_key=key_xyz)\n",
    "chat_model = ChatOpenAI(openai_api_key=key_xyz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#openai.api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Language Models Within LangChain: *LLMs and ChatModels*\n",
    "---\n",
    "\n",
    "#### Two Types of Language Models Within LangChain: \n",
    "- **LLMs**: input = string -> output = string \n",
    "    - to exit out `\"\\n\\n\"` from string when printing, use `[2:]`\n",
    "- **ChatModels**: input = list of `ChatMessage`s -> output = `ChatMessage`\n",
    "    - `ChatMessage` has two components: \n",
    "        - `content` *(content of message)* \n",
    "        - `role` *(role of entity the `ChatMessage` is coming from)*\n",
    "            > `role` Objects: \n",
    "            > - `HumanMessage` = from a Human/User \n",
    "            > - `AIMessage` = from AI/Assistant\n",
    "            > - `SystemMessage` = from system \n",
    "            > - `FunctionMessage` = from a function cell\n",
    "            > - custom role class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "example1 = \"Hello!\"\n",
    "example_sentence = \"What would be a good name for a company that made colorful socks?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `predict`: takes a string -> returns a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example string: Hello! \n",
      " ------ \n",
      " LLM: Welcome to the forum. How are you doing today? \n",
      " ------ \n",
      " ChatModel: Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "# predict: takes a string -> returns a string \n",
    "\n",
    "print(\"example string: {} \\n ------ \\n LLM: {} \\n ------ \\n ChatModel: {}\".format(example1,llm.predict(example1)[2:],chat_model.predict(example1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `predict_messages`: takes a list of messages -> returns a message\n",
    "- `ChatMessage`: \n",
    "    - `content` = string \n",
    "    - `role` = `HumanMessage` *from a human/user*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Input: [HumanMessage(content='What would be a good name for a company that made colorful socks?', additional_kwargs={}, example=False)] \n",
      " ------ \n",
      " LLM: content='\\n\\nSockorific!' additional_kwargs={} example=False \n",
      "   Sockorific! \n",
      " ------ \n",
      " ChatModel: content='Socktastic' additional_kwargs={} example=False \n",
      "   Socktastic\n"
     ]
    }
   ],
   "source": [
    "# predict_messages: takes a list of messages -> returns a message\n",
    "# HumanMessage role \n",
    "from langchain.schema import HumanMessage\n",
    "\n",
    "messages = [HumanMessage(content=example_sentence)]\n",
    "\n",
    "\n",
    "llm_string1 = llm.predict_messages(messages)\n",
    "cm_string1 = chat_model.predict_messages(messages)\n",
    "\n",
    "print(\"String Input: {} \\n ------ \\n LLM: {} \\n   {} \\n ------ \\n ChatModel: {} \\n   {}\".format(messages, llm_string1, llm_string1.content[2:], cm_string1, cm_string1.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates: \n",
    "---\n",
    "- most LLM appications do not pass user input directly into LLM\n",
    "    - will instead add user input to a larger piece of text, *prompt template* to provide additional context \n",
    "- `PromptTemplate` bundles logic from user input into a fully formatted prompt \n",
    "\n",
    "> Advantages Of `PromptTemplate` Over Raw String Formats: \n",
    "> - ability to \"partial\" out variables (focus on a single variable or a few)\n",
    "> - easy to combine templates into a single prompt  \n",
    "> - used to produce a list of messages (contains information about content but also each message) \n",
    ">   - common for a ChatPromptTemplate to be a list of ChatMessageTemplates each containing instructions for how to format the ChatMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is a good name for a company that makes colorful socks?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}?\")\n",
    "prompt.format(product=\"colorful socks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='You are a helpful assistant that translates English to French.', additional_kwargs={}),\n",
       " HumanMessage(content='I love programming!', additional_kwargs={}, example=False)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate, \n",
    "    SystemMessagePromptTemplate, \n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "\n",
    "human_template = \"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt,human_message_prompt])\n",
    "chat_prompt.format_messages(input_language=\"English\", output_language=\"French\", text=\"I love programming!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Parsers: \n",
    "---\n",
    "- convert raw output of a LLM into a format that can be used downstream \n",
    "\n",
    "> Main Types of `OutputParsers`: \n",
    "> - convert text from LLM -> structured information (ex: JSON)\n",
    "> - convert a `ChatMessage` into a string \n",
    "> - convert extra information returned from a call besides the message into a string "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi', 'how are ya', 'bye']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert comma separated list into a list \n",
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class CommaSeparatedListOutputParser(BaseOutputParser): \n",
    "    \"\"\"Parse Output of LLM Call Into A Comma-Separated List\"\"\"\n",
    "    def parse(self,text:str): \n",
    "        '''Parse Output of LLM Call'''\n",
    "        return text.strip().split(\", \")\n",
    "    \n",
    "CommaSeparatedListOutputParser().parse(\"hi, how are ya, bye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLMChain \n",
    "--- \n",
    "- combine everything into one chain, bundling a modular piece of logic \n",
    "- chain will accept input variables \n",
    "    - variables passed to a prompt template to create a prompt \n",
    "        - prompt passed to a LLM \n",
    "            - LLM output passed thrugh an *(optional)* output parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain Imports \n",
    "\n",
    "#from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.schema import BaseOutputParser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red', 'blue', 'green', 'yellow', 'orange']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class CommaSeparatedListOutputParser(BaseOutputParser): \n",
    "\n",
    "    def parse(self, text:str): \n",
    "        return text.strip().split(\", \")\n",
    "    \n",
    "template = \"\"\"You are a helpful assistant who generates comma separated lists. \n",
    "A user will pass in a category, and you should generate 5 objects in that category in a comma separated list. \n",
    "ONLY return a comma separated list, and nothing more.\"\"\"\n",
    "\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template) \n",
    "human_template = \"{text}\" \n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template) \n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "\n",
    "#FIX API KEY LATER \n",
    "chain = LLMChain(llm=ChatOpenAI(openai_api_key=key_xyz),\n",
    "                 prompt = chat_prompt, \n",
    "                 output_parser=CommaSeparatedListOutputParser()\n",
    "                 )\n",
    "\n",
    "\n",
    "chain.run(\"colors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
